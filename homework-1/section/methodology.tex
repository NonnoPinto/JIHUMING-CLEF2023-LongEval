\section{Methodology}
\label{sec:methodology}
In this section, we address how the system was developed in Java using the library Lucene.
Some parts of the code we are going to present are inspired by the repositories developed by Professor Nicola Ferro and
presented to us during the Search Engine course. \\
Furthermore, we will present the main methodology and approaches using the structure in the repository\cite{jihuming}.

\subsection{Parsing}
Participating in the LongEval CLEF Lab 2023 we are provided with a collection of French (original) and English
(translated from French) documents.
To generate an index based on them, first, we have to parse them;
this is, get their text into Java data structures.
All this parser package has been developed following the instructions provided in the dataset downloaded.\\
We decided to use the \textbf{JSON} version of the documents because they can easily be manipulated and queried using a
variety of tools and libraries.
Additionally, because of the large number of documents we are working with, we had to create a \textbf{steam} parser,
allocating into the main memory one document at a time.
Thus, our parser is based on the Java library \textttt{Gson}, especially in the stream and JSON functions and classes.\\
The whole parser is made up of:
\begin{itemize}
    \item \texttt{DocumentParser}: an abstract class representing a stream parser for documents in a collection.
    \item \texttt{JsonDocument}: a Java POJO used in the deserialization of JSON documents.
    \item \texttt{ParsedDocument}: Java object that represents a document already parsed.
Note that this object contains an identifier and a body.
    \item \texttt{LongEvalParser}: real parser implementing the class \texttt{DocumentParser}.
Here is where the stream logic is implemented.
Objects of this class can be used as iterators returning parsed documents.
\end{itemize}

\subsection{Analyzer}
As basis for the analyzer we used the TokenStream class from Lucene, which 
Consumes a TokenStream for the given text by using the provided
Analyzer and prints diagnostic information about all the generated tokens and
their Attributes.\\
Streams are analyzed by applying these filters:
\begin{itemize}
    \item \texttt{WhitespaceTokenizer}: splits on and discards only
    whitespace characters %TODO: initizlized but never used
    \item \texttt{PatternReplaceFilter}: it's applied twice. First time, using RegExpr patter deletes punctuations
    marks at the beginning of tokens, second time it does the same but at the end of them
    \item \texttt{WordDelimiterGraphFilter}: it splits words into subwords and performs optional transformations on subword groups.
        In our case, we decided to use these filters:
    \begin{lstlisting}[language=Java]
WordDelimiterGraphFilter.GENERATE_WORD_PARTS
                    // Ex: "PowerShot" => "Power" "Shot"
| WordDelimiterGraphFilter.GENERATE_NUMBER_PARTS
                    // Ex: "500-42" => "500" "42"
| WordDelimiterGraphFilter.CATENATE_NUMBERS
                    // Ex: "500-42" => "50042"
| WordDelimiterGraphFilter.PRESERVE_ORIGINAL
                    // Ex: "500-42" => "500" "42" "500-42"
| WordDelimiterGraphFilter.SPLIT_ON_CASE_CHANGE
                    // Causes lowercase -> uppercase transition to start a new subword.
| WordDelimiterGraphFilter.STEM_ENGLISH_POSSESSIVE
                    // "O'Neil's" => "O", "Neil"
    \end{lstlisting}
    \item \texttt{LowerCaseFilter}: converts all characters to lowercase %TODO: are we checking for BRAND names? (apple == Apple?)
    \item \texttt{StopFilter}: removes stop words applying terrier list\cite{stopword}
    \item \texttt{SynonymTokenFilter}: only for English analyzer, it uses WordNet lexical database\cite{wordnet} to group words
        interlinked by means of conceptual-semantic and lexical relations
    \item \texttt{MinimalStemFilter}: implementing S-Stemmer from Harman article\cite{engStemFilter} for English language
        and Savoy stemming procedure\cite{frStemFilter}, it divides words into stems
    \item \texttt{EmptyTokenFilter}: removes tokens with length 0
\end{itemize}
    Lastly, the \texttt{NGramAnalyzer} is applied, trying tokens of one, two and three words. We decided to set the max at three,
    but this number is heuristic and can be changed if results shows better numbers.\\

\subsection{Index}
We used the standard Lucene Indexer with the BM25\cite{BM25} similarity.\\
%TODO: explain the deprecated indexers
We decide to use NGram analyzer with a multilingual indexer,
in order to index two version of the same document, creating documents with
an unique ID and bodies from each language.\\
Ngrams are represented as characters from both English and French version of
the documents. N parameter is set at the analyzer level and class is a field,
not stored, in order to minimize space occupation.\\
%TODO: still lot to write

\subsection{Search}

\subsection{Topic}